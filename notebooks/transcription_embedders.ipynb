{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Habilitar la extensión de vectores\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "-- Crear tabla para almacenar embeddings\n",
    "CREATE TABLE podcast_embeddings (\n",
    "    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "    content TEXT NOT NULL,\n",
    "    metadata JSONB,\n",
    "    embedding vector(1536), -- text-embedding-3-small usa 1536 dimensiones\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Función corregida que usa la tabla podcast_embeddings\n",
    "create or replace function match_documents (\n",
    "  query_embedding vector (1536),\n",
    "  filter jsonb default '{}',\n",
    "  match_count int default 5\n",
    ") returns table (\n",
    "  id uuid,\n",
    "  content text,\n",
    "  metadata jsonb,\n",
    "  similarity float\n",
    ") language plpgsql as $$\n",
    "#variable_conflict use_column\n",
    "begin\n",
    "  return query\n",
    "  select\n",
    "    podcast_embeddings.id,\n",
    "    podcast_embeddings.content,\n",
    "    podcast_embeddings.metadata,\n",
    "    1 - (podcast_embeddings.embedding <=> query_embedding) as similarity\n",
    "  from podcast_embeddings\n",
    "  where podcast_embeddings.metadata @> filter\n",
    "  order by podcast_embeddings.embedding <=> query_embedding\n",
    "  limit match_count;\n",
    "end;\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunker:\n",
    "    \"\"\"Chunker semántico especializado para transcripciones de podcast\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"],\n",
    "            keep_separator=True\n",
    "        )\n",
    "    \n",
    "    def detect_topic_boundaries(self, text: str) -> List[int]:\n",
    "        \"\"\"Detecta límites de temas basado en patrones comunes en podcasts\"\"\"\n",
    "        boundaries = [0]\n",
    "        \n",
    "        # Patrones que indican cambio de tema\n",
    "        patterns = [\n",
    "            r'\\b(?:ahora|bueno|entonces|por otro lado|cambiando de tema|hablando de)\\b',\n",
    "            r'\\b(?:siguiente pregunta|otra cosa|pasemos a|vamos a hablar)\\b',\n",
    "            r'\\n\\n',  # Párrafos nuevos\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "            for match in matches:\n",
    "                boundaries.append(match.start())\n",
    "        \n",
    "        # Ordenar y eliminar duplicados\n",
    "        boundaries = sorted(list(set(boundaries)))\n",
    "        boundaries.append(len(text))\n",
    "        \n",
    "        return boundaries\n",
    "    \n",
    "    def chunk_transcript(self, text: str, episode_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Divide la transcripción en chunks semánticamente coherentes\"\"\"\n",
    "        boundaries = self.detect_topic_boundaries(text)\n",
    "        chunks = []\n",
    "        \n",
    "        # Estimar duración total (asumiendo ~150 palabras por minuto)\n",
    "        total_words = len(text.split())\n",
    "        estimated_duration_minutes = total_words / 150\n",
    "        \n",
    "        for i in range(len(boundaries) - 1):\n",
    "            start_pos = boundaries[i]\n",
    "            end_pos = boundaries[i + 1]\n",
    "            \n",
    "            segment = text[start_pos:end_pos].strip()\n",
    "            \n",
    "            if len(segment) < 100:  # Skip chunks muy pequeños\n",
    "                continue\n",
    "            \n",
    "            # Si el segmento es muy largo, usar text_splitter tradicional\n",
    "            if len(segment) > self.chunk_size:\n",
    "                sub_chunks = self.text_splitter.split_text(segment)\n",
    "                \n",
    "                for j, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Calcular timestamp estimado\n",
    "                    progress = (start_pos + j * len(sub_chunk)) / len(text)\n",
    "                    estimated_timestamp = progress * estimated_duration_minutes\n",
    "                    \n",
    "                    chunk_metadata = {\n",
    "                        **episode_metadata,\n",
    "                        \"chunk_index\": len(chunks),\n",
    "                        \"estimated_timestamp_minutes\": round(estimated_timestamp, 2),\n",
    "                        \"chunk_type\": \"semantic_sub\",\n",
    "                        \"word_count\": len(sub_chunk.split())\n",
    "                    }\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        \"content\": sub_chunk,\n",
    "                        \"metadata\": chunk_metadata\n",
    "                    })\n",
    "            else:\n",
    "                # Calcular timestamp estimado\n",
    "                progress = start_pos / len(text)\n",
    "                estimated_timestamp = progress * estimated_duration_minutes\n",
    "                \n",
    "                chunk_metadata = {\n",
    "                    **episode_metadata,\n",
    "                    \"chunk_index\": len(chunks),\n",
    "                    \"estimated_timestamp_minutes\": round(estimated_timestamp, 2),\n",
    "                    \"chunk_type\": \"semantic\",\n",
    "                    \"word_count\": len(segment.split())\n",
    "                }\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"content\": segment,\n",
    "                    \"metadata\": chunk_metadata\n",
    "                })\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastIndexer:\n",
    "    \"\"\"Sistema principal de indexación para transcripciones de podcast\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Configurar Supabase\n",
    "        supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "        supabase_key = os.getenv(\"SUPABASE_KEY\")\n",
    "        \n",
    "        if not supabase_url or not supabase_key:\n",
    "            raise ValueError(\"SUPABASE_URL y SUPABASE_KEY deben estar configurados\")\n",
    "        \n",
    "        self.supabase_client: Client = create_client(supabase_url, supabase_key)\n",
    "        self.vector_store = SupabaseVectorStore(\n",
    "            client=self.supabase_client,\n",
    "            embedding=self.embeddings,\n",
    "            table_name=\"podcast_embeddings\",\n",
    "            query_name=\"match_documents\"\n",
    "        )\n",
    "        self.chunker = SemanticChunker()\n",
    "    \n",
    "    def extract_episode_metadata(self, file_path: Path) -> Dict[str, Any]:\n",
    "        \n",
    "        filename = file_path.stem\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error leyendo JSON {file_path}: {str(e)}\")\n",
    "            data = {}\n",
    "        \n",
    "        episode_id = data.get(\"episode_id\", filename)\n",
    "        title = data.get(\"title\", episode_id)\n",
    "        duration = data.get(\"duration\", 0)\n",
    "        language = data.get(\"language\", \"unknown\")\n",
    "        audio_file_path = data.get(\"file_path\", \"\")\n",
    "        \n",
    "        return {\n",
    "            \"episode_id\": episode_id,\n",
    "            \"title\": title,\n",
    "            \"filename\": filename,\n",
    "            \"source_file\": str(file_path),\n",
    "            \"duration\": duration,\n",
    "            \"language\": language,\n",
    "            \"file_path\": audio_file_path\n",
    "        }\n",
    "\n",
    "    \n",
    "    def process_transcript_file(self, file_path: Path) -> List[Dict[str, Any]]:\n",
    "        logger.info(f\"Procesando: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            episode_metadata = self.extract_episode_metadata(file_path)\n",
    "            chunks = self.chunker.chunk_transcript(content, episode_metadata)\n",
    "            \n",
    "            logger.info(f\"Generados {len(chunks)} chunks para {file_path.name}\")\n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error procesando {file_path}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def index_transcripts(self, transcripts_dir: str = \"transcripciones\"):\n",
    "        transcripts_path = Path(transcripts_dir)\n",
    "        \n",
    "        if not transcripts_path.exists():\n",
    "            raise FileNotFoundError(f\"Directorio {transcripts_dir} no encontrado\")\n",
    "        \n",
    "        transcript_files = list(transcripts_path.glob(\"*.json\"))\n",
    " \n",
    "        \n",
    "        # Seleccionar solo 2 archivos aleatorios\n",
    "        # transcript_files = random.sample(transcript_files, min(2, len(transcript_files)))\n",
    "        \n",
    "        if not transcript_files:\n",
    "            logger.warning(f\"No se encontraron archivos .txt en {transcripts_dir}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Encontrados {len(transcript_files)} archivos de transcripción\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        # Procesar cada archivo\n",
    "        for file_path in transcript_files:\n",
    "            chunks = self.process_transcript_file(file_path)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        if not all_chunks:\n",
    "            logger.warning(\"No se generaron chunks para indexar\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Total de chunks a indexar: {len(all_chunks)}\")\n",
    "        \n",
    "        texts = [chunk[\"content\"] for chunk in all_chunks]\n",
    "        metadatas = [chunk[\"metadata\"] for chunk in all_chunks]\n",
    "        \n",
    "        batch_size = 50\n",
    "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_metadatas = metadatas[i:i + batch_size]\n",
    "            current_batch = (i // batch_size) + 1\n",
    "            \n",
    "            try:\n",
    "                self.vector_store.add_texts(\n",
    "                    texts=batch_texts,\n",
    "                    metadatas=batch_metadatas\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"Batch {current_batch}/{total_batches} indexado exitosamente\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error indexando batch {current_batch}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(\"Indexación completada!\")\n",
    "    \n",
    "    def search_episodes(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            logger.info(f\"Buscando: '{query}' con k={k}\")\n",
    "            \n",
    "            # Verificar conexión a Supabase\n",
    "            logger.info(\"Verificando conexión a Supabase...\")\n",
    "            \n",
    "            # Intentar búsqueda\n",
    "            docs = self.vector_store.similarity_search(query=query, k=k)\n",
    "            logger.info(f\"Búsqueda exitosa, encontrados {len(docs)} documentos\")\n",
    "            \n",
    "            results = []\n",
    "            for doc in docs:\n",
    "                result = {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            logger.error(f\"Error en búsqueda: {str(e)}\")\n",
    "            logger.error(f\"Tipo de error: {type(e).__name__}\")\n",
    "            logger.error(f\"Traceback completo:\\n{traceback.format_exc()}\")\n",
    "            \n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten cuidad con ejecutar de nuevo. 🚨\n",
    "indexer = PodcastIndexer()\n",
    "# indexer.index_transcripts(\"../data/transcriptions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:41:31,684 - INFO - Buscando: 'Iglesia catolica' con k=5\n",
      "2025-07-06 19:41:31,685 - INFO - Verificando conexión a Supabase...\n",
      "2025-07-06 19:41:32,292 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 19:41:32,663 - INFO - HTTP Request: POST https://zdanxslxyfhuvwqczyig.supabase.co/rest/v1/rpc/match_documents?limit=5 \"HTTP/2 200 OK\"\n",
      "2025-07-06 19:41:32,674 - INFO - Búsqueda exitosa, encontrados 5 documentos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'ahora viene la extravagancia Ya sabemos todos que la secta católica tiene por norma trapichear con huesos de muertos para ponerlos debajo de sus altares Durante la inauguración de esta sede del Opus en el CSIC buscaron los huesos más oportunos, dado que habían declarado patrón espiritual al tal Isidoro de Sevilla Oye,',\n",
       "  'metadata': {'title': '20240520_190000',\n",
       "   'duration': 925,\n",
       "   'filename': '20240520_190000',\n",
       "   'language': 'es',\n",
       "   'file_path': '/Users/seedtag/projects/personal/la-historia-por-concostrina/podcast_crawler/app/../../audios/2024_05_20_19.mp3',\n",
       "   'chunk_type': 'semantic',\n",
       "   'episode_id': '20240520_190000',\n",
       "   'word_count': 53,\n",
       "   'chunk_index': 12,\n",
       "   'source_file': '../data/transcriptions/20240520_190000.json',\n",
       "   'estimated_timestamp_minutes': 10.06}},\n",
       " {'content': 'bueno, hay que recordar que la república no tuvo oportunidad de desarrollar plenamente esa constitución aprobada en el 31, ni de secularizar la enseñanza, porque lo impidió un golpe de estado. O sea, todo eso que estaba previsto de hacer, ta ta ta, pum, de repente se frenó de golpe. Había un buen plan, pero que no se pudo desarrollar nada. Es que no se pudo, no se pudo. Era una constitución muy progresista, pero se frenó todo. Pero por lo de siempre, la alianza del trono, el altar y el fascismo, esa alianza lo impidió. En este país no hay forma de sacudirse de encima a una iglesia católica que además de no contribuir estorba, que es lo peor. No paga y exige. Se salta todos los mandamientos, pero exigen que los cumplan sus clientes. Y están tan mal acostumbrados y han abusado tanto a lo largo de los siglos que aún hoy se sienten con derecho a todo, porque hasta los gobiernos que nos quieren hacer creer que son progresistas siguen regando a una jerarquía eclesiástica de dinero público cuando nos falta dinero para la investigación. Ya se escuchó esta tarde a Isaías, cuando hablábamos de ese convento de Sevilla, de donde van a...',\n",
       "  'metadata': {'title': '20240206_193433',\n",
       "   'duration': 857,\n",
       "   'filename': '20240206_193433',\n",
       "   'language': 'es',\n",
       "   'file_path': '/Users/seedtag/projects/personal/la-historia-por-concostrina/podcast_crawler/app/../../audios/2024_02_06_19.mp3',\n",
       "   'chunk_type': 'semantic',\n",
       "   'episode_id': '20240206_193433',\n",
       "   'word_count': 206,\n",
       "   'chunk_index': 11,\n",
       "   'source_file': '../data/transcriptions/20240206_193433.json',\n",
       "   'estimated_timestamp_minutes': 10.93}},\n",
       " {'content': '. Por aquel pueblo pasó cuatro años después dando la turra con el evangelio el franciscano Fray Toribio que se enteró de que allí estaba enterrado el celebrado Cuauhtémoc y decidió construir sobre su tumba la iglesia de Nuestra Señora de la Asunción Eso es tener puntería Sí, claro, a falta de un santo te pillan a un azteca y dices con tal de construir algo yo un gamusín a un azteca da lo mismo como si fuera San Pedro el caso es que este hombre tiene allí una iglesia Yo esto me lo creo porque los curas cuando van a abrir nueva línea de negocio inauguran nueva sede de la multinacional con la excusa de cualquier muerto, les da igual y si esto fuera cierto hay que ver la idiotez, si Cuauhtémoc el último emperador mexica asesinado por los católicos estuviera enterrado bajo el altar mayor de una sucursal católica se quejan los mexicanos de los invasores españoles con razón, yo lo entiendo, pero la peor consecuencia visto',\n",
       "  'metadata': {'title': '20240228_191812',\n",
       "   'duration': 1050,\n",
       "   'filename': '20240228_191812',\n",
       "   'language': 'es',\n",
       "   'file_path': '/Users/seedtag/projects/personal/la-historia-por-concostrina/podcast_crawler/app/../../audios/2024_02_28_19.mp3',\n",
       "   'chunk_type': 'semantic_sub',\n",
       "   'episode_id': '20240228_191812',\n",
       "   'word_count': 169,\n",
       "   'chunk_index': 13,\n",
       "   'source_file': '../data/transcriptions/20240228_191812.json',\n",
       "   'estimated_timestamp_minutes': 11.2}},\n",
       " {'content': 'bueno, ya han hecho cuatro apartamentos turísticos dentro del convento, y Isaías ha comentado, oye, pues por ahí podría abrirse una vía para que con todos los inmuebles que tiene la iglesia algún día pudiera autofinanciarse, a lo que se comprometieron hace un montón de años. Oye, igual por ahí, yo no creo, pero igual por ahí suena la flauta. Nada, ni suena la flauta porque estos no tocan la flauta, sino es para adentro. Perdimos el paso del progreso en el 19 y todo fue a peor, empeoró muchísimo a partir de 1851, cuando el papa Pío Nono... El pastelito. Pues cuando Pío Nono y la reina Isabel II, esa de la que el propio papa decía, e putana ma pía, es una puta pero es devota. Sí, eso decía, le dieron un premio y alguien le dijo, pero si Isabel II es una puta, le dijo un cardenal y dijo, e putana ma pía. Pues desde que estordó, os digo, firmaron el concordato que reconocía el derecho de la iglesia a inspeccionar la enseñanza tanto pública como privada. Eso es de 1851 y eso que no hemos contado que aún faltaba por desarrollar la ley para respaldar los puntos cuarto y quinto del artículo 26 de la Constitución del 31, que prohibía a todas las órdenes religiosas ejercer la industria, el comercio y la enseñanza y estaban obligadas a someterse a todas las leyes tributarias del país. Esa era la buena constitución',\n",
       "  'metadata': {'title': '20240206_193433',\n",
       "   'duration': 857,\n",
       "   'filename': '20240206_193433',\n",
       "   'language': 'es',\n",
       "   'file_path': '/Users/seedtag/projects/personal/la-historia-por-concostrina/podcast_crawler/app/../../audios/2024_02_06_19.mp3',\n",
       "   'chunk_type': 'semantic_sub',\n",
       "   'episode_id': '20240206_193433',\n",
       "   'word_count': 244,\n",
       "   'chunk_index': 12,\n",
       "   'source_file': '../data/transcriptions/20240206_193433.json',\n",
       "   'estimated_timestamp_minutes': 12.25}},\n",
       " {'content': '. Pero claro, había excepciones muy razonables a las que nadie prestaba atención, porque aquí lo que se trataba era de montar el pollo y dar la bronca. Esa es una de las excepciones. Si todos los padres y el profesor estaban de acuerdo, el crucifijo podía quedarse en el aula. Todos estaban todos de acuerdo. Y si los padres quisieran que sus hijos recibieran clases de religión, pues se respetaría. Pero claro, tenían que estar todos de acuerdo. Como un extraescolar, parece. Claro. Eso era, pues eso, al margen de las materias importantes y serias. Ustedes quieren esto, pues se puede mantener, pero tienen que estar todos de acuerdo. Pero el caso es que a partir de aquí los curas montaron sus performance a lo largo y ancho del territorio la ICO Nacional. ¿Cómo? ¿Con qué? ¿Procesiones? ¿Manifestaciones? ¿Encierros? ¿Burgos de hambre? De todo un poco. Y visto desde la distancia, pues algunas eran auténticas payasadas, aunque no tenían maldita gracia. Los curas iban organizando protestas o procesiones por pueblos y ciudades. Cada día la liaban en un lugar. Ejemplos prácticos. El cura de un pueblo de Granada que se llama Aledín montó una procesión con una asociación agraria de católicos, una procesión aparentemente piadosa, a la que arrastraron a los vecinos que no se enteran que cuando un cura te movilices para defender lo suyo. Defiende su buche, su negocio, sus ingresos. No defiende las necesidades del pueblo. Pues aquella procesión se volvió violenta',\n",
       "  'metadata': {'title': '20240206_193433',\n",
       "   'duration': 857,\n",
       "   'filename': '20240206_193433',\n",
       "   'language': 'es',\n",
       "   'file_path': '/Users/seedtag/projects/personal/la-historia-por-concostrina/podcast_crawler/app/../../audios/2024_02_06_19.mp3',\n",
       "   'chunk_type': 'semantic_sub',\n",
       "   'episode_id': '20240206_193433',\n",
       "   'word_count': 246,\n",
       "   'chunk_index': 6,\n",
       "   'source_file': '../data/transcriptions/20240206_193433.json',\n",
       "   'estimated_timestamp_minutes': 4.96}}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = PodcastIndexer()\n",
    "indexer.search_episodes(\"Iglesia catolica\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la-historia-por-concostrina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
